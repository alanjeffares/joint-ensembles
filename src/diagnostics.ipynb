{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fQi0EgmRSDW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import numpy as np\n",
        "import yaml\n",
        "import os\n",
        "from sklearn import datasets\n",
        "import matplotlib.gridspec as gridspec\n",
        "cwd = os.getcwd()\n",
        "\n",
        "green = plt.cm.Greens(0.2*(6/4) + 0.4)\n",
        "blue = plt.cm.Blues(0.2*(6/4) + 0.4)\n",
        "red = plt.cm.Reds(0.2*(6/4) + 0.4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# note that the outputs of the Bayesian optimization runs are stored at the following link:\n",
        "\n",
        "# https://drive.google.com/drive/folders/1WN8uEkxRbyV5DAnJuZWfvAN0BRyJkbLl?usp=sharing\n",
        "\n",
        "# once downloaded, we perform the three diagnostic experiments in this notebook\n",
        "\n"
      ],
      "metadata": {
        "id": "lx7ZHu8XSqoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_results_filename(model, dataset, path='path/to/data'):\n",
        "    local_files = os.listdir(path)\n",
        "    for file_ in local_files:\n",
        "        if file_.startswith(f'{model}_{dataset}') and not file_.endswith('config.json'):\n",
        "            return file_\n"
      ],
      "metadata": {
        "id": "hycWzuJQRhus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = ['bioconcentration', 'weather', 'facebook', 'abalone']\n",
        "\n",
        "\n",
        "\n",
        "model = 'deep_ensemble'\n",
        "path = 'data'\n",
        "\n",
        "\n",
        "title_size = 22\n",
        "label_size = 22\n",
        "tick_size = 20\n",
        "\n",
        "fig = plt.figure(figsize=(20, 3.5))\n",
        "outer = gridspec.GridSpec(1, 4, wspace=0.2, hspace=0.4)\n",
        "\n",
        "# per dataset\n",
        "for i in range(4):\n",
        "\n",
        "    dataset = datasets[i]\n",
        "    print(dataset)\n",
        "    filename = get_results_filename(model, dataset, path)\n",
        "    with open(f'{path}/{filename}') as f:\n",
        "        results = json.load(f)\n",
        "\n",
        "    betas = []\n",
        "    eval_losses = []\n",
        "    div_losses = []\n",
        "    w_ind_losses = []\n",
        "    for run in results.keys():\n",
        "        eval_losses.append(results[run]['ens_loss'])\n",
        "        div_losses.append(results[run]['div_loss'])\n",
        "        w_ind_losses.append(results[run]['w_ind_loss'])\n",
        "    inner = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec=outer[i],\n",
        "                                             wspace=0.15, hspace=0.25)\n",
        "\n",
        "    # loss vs diversity\n",
        "    for j in range(2):\n",
        "\n",
        "        # plot loss\n",
        "        ax = plt.Subplot(fig, inner[0])\n",
        "        ax.set_title(dataset.capitalize(), fontsize=title_size)\n",
        "        m = np.mean(eval_losses, axis=0)\n",
        "        s = np.std(eval_losses, axis=0)\n",
        "        ax.plot(results[run]['beta'], m, color=red, marker='o', ms=3)\n",
        "        ax.fill_between(results[run]['beta'], m - s, y2=m + s, color=red,\n",
        "                            alpha=0.25)\n",
        "        ax.set_xticks([])\n",
        "        upper = max(m + s)\n",
        "        lower = min(m-s)\n",
        "        eps = (upper - lower)/10\n",
        "        y_ticks = [lower, (lower+upper)/2, upper]\n",
        "        ax.set_ylim([lower - eps, upper + eps])\n",
        "        ax.set_yticks(y_ticks)\n",
        "        ax.set_yticklabels([round(tick, 3) for tick in y_ticks])\n",
        "        if i == 0:\n",
        "          ax.set_ylabel('MSE', fontsize=label_size)\n",
        "          ax.get_yaxis().set_label_coords(-0.21,0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=tick_size)\n",
        "        fig.add_subplot(ax)\n",
        "\n",
        "\n",
        "        # plot diversity\n",
        "        ax = plt.Subplot(fig, inner[1])\n",
        "        m = np.mean(div_losses, axis=0)\n",
        "        s = np.std(div_losses, axis=0)\n",
        "        ax.plot(results[run]['beta'], m, color=green, marker='o', ms=3)\n",
        "        ax.fill_between(results[run]['beta'], m - s, y2=m + s, color=green,\n",
        "                            alpha=0.25)\n",
        "        upper = max(m + 0.2*s)\n",
        "        lower = max([min(m-s),0])\n",
        "        eps = (upper - lower)/10\n",
        "        y_ticks = [lower, (lower+upper)/2, upper]\n",
        "        ax.set_ylim([lower - eps, upper + eps])\n",
        "        ax.set_yticks(y_ticks)\n",
        "        ax.set_yticklabels([round(tick, 3) for tick in y_ticks])\n",
        "        ax.set_xticks([0.0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "        if i == 0:\n",
        "          ax.set_ylabel('Diversity', fontsize=label_size)\n",
        "          ax.get_yaxis().set_label_coords(-0.21,0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=tick_size)\n",
        "        # if i > 1 and j ==1:\n",
        "        ax.set_xlabel(r'$\\beta$', fontsize=label_size+3)\n",
        "        fig.add_subplot(ax)\n",
        "\n",
        "plt.savefig(f'Diversity_loss_{model}_neurips.pdf', dpi=1200)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "hhYpx48XRmfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dnqZTz5dTNJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# performs the dropout experiment and plots results\n",
        "\n",
        "def shuffle_along_axis(a, axis):\n",
        "    idx = np.random.rand(*a.shape).argsort(axis=axis)\n",
        "    return np.take_along_axis(a,idx,axis=axis)\n",
        "\n",
        "# run dropout experiment\n",
        "props = [0.2, 0.4, 0.6]\n",
        "n_iter = 100\n",
        "\n",
        "\n",
        "title_size = 22\n",
        "label_size = 22\n",
        "tick_size = 20\n",
        "lwd = 3\n",
        "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(17,3))\n",
        "for d, dataset in enumerate(datasets):\n",
        "    print(dataset)\n",
        "    filename = get_results_filename(model, dataset, path)\n",
        "    with open(f'{path}/{filename}') as f:\n",
        "        results = json.load(f)\n",
        "    ens_loss_drop_full_ls = [[] for _ in props]\n",
        "\n",
        "    ens_loss_b_full_ls = []\n",
        "\n",
        "    mse_full_ls = []\n",
        "    for run in results.keys():\n",
        "      # ens_loss_drop_ls = []\n",
        "      ens_loss_drop_ls = [[] for _ in props]\n",
        "      ens_loss_b_ls = []\n",
        "\n",
        "      mse_ls = []\n",
        "      labels = np.array(results[run]['targets'])[0]\n",
        "      ind_preds = np.array(results[run]['ind_preds']) # shape = (num_beta_vals, test_set_size, ensemble_size)\n",
        "      for idx, beta in enumerate(results[run]['beta']):\n",
        "        # get decomposition of biased preds for this value of beta\n",
        "        biased_preds_ind = ind_preds[idx]\n",
        "        biased_preds_ens = biased_preds_ind.mean(1)\n",
        "        ens_losses_b = (biased_preds_ens - labels)**2\n",
        "        w_ind_losses_b = np.mean((biased_preds_ind[:,:,0] - labels[:,None])**2, axis=1)\n",
        "        divs_b = w_ind_losses_b - ens_losses_b\n",
        "        ens_loss_b_ls.append(np.mean(ens_losses_b))\n",
        "\n",
        "\n",
        "        # get mse of raw preds\n",
        "        mse = np.mean((ind_preds[idx].mean(1) - labels)**2)\n",
        "        mse_ls.append(mse)\n",
        "\n",
        "        # get dropout preds\n",
        "        # for each drop proportion being considered\n",
        "        for c, drop_prop in enumerate(props):\n",
        "            num_drop = int(biased_preds_ind.shape[1] * drop_prop)\n",
        "            num_keep = biased_preds_ind.shape[1] - num_drop\n",
        "            drop_prop_losses = []\n",
        "            # run n_iter iterations\n",
        "            for i in range(n_iter):\n",
        "                mask = np.ones(biased_preds_ind.shape)\n",
        "                mask[:,:num_drop] = 0\n",
        "                mask = shuffle_along_axis(mask, 1)\n",
        "                dropped_ind_preds = mask * biased_preds_ind\n",
        "                dropped_ens_pred = dropped_ind_preds.sum(1)/num_keep\n",
        "                ens_losses_drop = (dropped_ens_pred - labels)**2\n",
        "                drop_prop_losses.append(np.mean(ens_losses_drop)) # avg test set loss for this iter\n",
        "            ens_loss_drop_ls[c].append(np.mean(drop_prop_losses)) # mean avg test set loss across all iter\n",
        "\n",
        "\n",
        "      # store results for this run\n",
        "      ens_loss_b_full_ls.append(ens_loss_b_ls)\n",
        "      for c in range(len(props)):\n",
        "        ens_loss_drop_full_ls[c].append(ens_loss_drop_ls[c])\n",
        "\n",
        "      mse_full_ls.append(mse_ls)\n",
        "\n",
        "    # plot results for this dataset\n",
        "    curr_ax = ax[d]\n",
        "    curr_ax.set_title(dataset.capitalize(), fontsize=title_size)\n",
        "    for c, prop in enumerate(props):\n",
        "        # m_drop = np.mean(ens_loss_drop_full_ls[c], axis=0)\n",
        "        change = np.array(ens_loss_drop_full_ls[c]) - np.array(ens_loss_b_full_ls)\n",
        "        m_drop = np.mean(change, axis=0)\n",
        "        max_ = max(props)\n",
        "        min_ = min(props)\n",
        "        upper = max_ - min_\n",
        "        col = plt.cm.Greens((prop-min_)*((1-upper)/upper) + upper)\n",
        "        min_col = plt.cm.Greens(((max_ - min_)/2)*((1-upper)/upper) + upper)\n",
        "        s_drop = np.std(change, axis=0)\n",
        "        curr_ax.plot(results[run]['beta'], m_drop, color=col, label=f'With {prop} dropout', linewidth=lwd)\n",
        "        curr_ax.fill_between(results[run]['beta'], m_drop - s_drop, y2=m_drop + s_drop, color=min_col,\n",
        "                            alpha=0.2)\n",
        "        upper = max(m_drop + s_drop)\n",
        "        lower = 0\n",
        "        eps = (upper - lower)/10\n",
        "        y_ticks = [lower, (lower+upper)/2, upper]\n",
        "        curr_ax.set_ylim([lower - eps, upper + eps])\n",
        "        curr_ax.set_yticks(y_ticks)\n",
        "        curr_ax.set_yticklabels([round(tick, 2) for tick in y_ticks])\n",
        "\n",
        "    curr_ax.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    if d == 0:\n",
        "      curr_ax.set_ylabel('Increase in\\ntest error', fontsize=label_size)\n",
        "      curr_ax.get_yaxis().set_label_coords(-0.15,0.5)\n",
        "    curr_ax.tick_params(axis='both', which='major', labelsize=tick_size)\n",
        "\n",
        "    curr_ax.set_xlabel(r'$\\beta$', fontsize=label_size+3)\n",
        "\n",
        "    if d == 0:\n",
        "      leg = curr_ax.legend(loc=(0.02,0.3), prop={'size': 18})\n",
        "      for line in leg.get_lines():\n",
        "          line.set_linewidth(4)\n",
        "\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.savefig(f'Dropout_{model}_neurips.pdf', dpi=1200)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p2yd6cc_Tqki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6xSPquKETsqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# performs\n",
        "\n",
        "title_size = 22\n",
        "label_size = 22\n",
        "tick_size = 20\n",
        "lwd = 3\n",
        "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(17, 3))\n",
        "for d, dataset in enumerate(datasets):\n",
        "    filename = get_results_filename(model, dataset, path)\n",
        "    with open(f'{path}/{filename}') as f:\n",
        "        results = json.load(f)\n",
        "    ens_loss_db_full_ls = []\n",
        "    ind_loss_db_full_ls = []\n",
        "    div_db_full_ls = []\n",
        "\n",
        "    ens_loss_b_full_ls = []\n",
        "    ind_loss_b_full_ls = []\n",
        "    div_b_full_ls = []\n",
        "\n",
        "    mse_full_ls = []\n",
        "    for run in results.keys():\n",
        "      ens_loss_db_ls = []\n",
        "      ind_loss_db_ls = []\n",
        "      div_db_ls = []\n",
        "\n",
        "      ens_loss_b_ls = []\n",
        "      ind_loss_b_ls = []\n",
        "      div_b_ls = []\n",
        "\n",
        "      mse_ls = []\n",
        "      labels = np.array(results[run]['targets'])[0]\n",
        "      ind_preds = np.array(results[run]['ind_preds'])[:,:,:,0] # shape = (num_beta_vals, test_set_size, ensemble_size)\n",
        "      for idx, beta in enumerate(results[run]['beta']):\n",
        "        # get decomposition of debiased preds for this value of beta\n",
        "        ind_bias = ind_preds[idx].mean(0) - ind_preds[idx].mean(0).mean()\n",
        "        debiased_preds_ind = ind_preds[idx] - ind_bias\n",
        "        debiased_preds_ens = debiased_preds_ind.mean(1)\n",
        "        ens_losses_db = (debiased_preds_ens - labels)**2\n",
        "        w_ind_losses_db = np.mean((debiased_preds_ind - labels[:,None])**2, axis=1)\n",
        "        divs_db = w_ind_losses_db - ens_losses_db\n",
        "        ens_loss_db_ls.append(np.mean(ens_losses_db))\n",
        "        ind_loss_db_ls.append(np.mean(w_ind_losses_db))\n",
        "        div_db_ls.append(np.mean(divs_db))\n",
        "\n",
        "        # get decomposition of biased preds for this value of beta\n",
        "        biased_preds_ind = ind_preds[idx]\n",
        "        biased_preds_ens = biased_preds_ind.mean(1)\n",
        "        ens_losses_b = (biased_preds_ens - labels)**2\n",
        "        w_ind_losses_b = np.mean((biased_preds_ind - labels[:,None])**2, axis=1)\n",
        "        divs_b = w_ind_losses_b - ens_losses_b\n",
        "        ens_loss_b_ls.append(np.mean(ens_losses_b))\n",
        "        ind_loss_b_ls.append(np.mean(w_ind_losses_b))\n",
        "        div_b_ls.append(np.mean(divs_b))\n",
        "\n",
        "        # get mse of raw preds\n",
        "        mse = np.mean((ind_preds[idx].mean(1) - labels)**2)\n",
        "        mse_ls.append(mse)\n",
        "\n",
        "      # store results for this run\n",
        "      ens_loss_db_full_ls.append(ens_loss_db_ls)\n",
        "      ind_loss_db_full_ls.append(ind_loss_db_ls)\n",
        "      div_db_full_ls.append(div_db_ls)\n",
        "\n",
        "      ens_loss_b_full_ls.append(ens_loss_b_ls)\n",
        "      ind_loss_b_full_ls.append(ind_loss_b_ls)\n",
        "      div_b_full_ls.append(div_b_ls)\n",
        "\n",
        "      mse_full_ls.append(mse_ls)\n",
        "\n",
        "    # plot results for this dataset\n",
        "    curr_ax = ax[d]\n",
        "    curr_ax.set_title(dataset.capitalize(), fontsize=title_size)\n",
        "\n",
        "    percent_decrease = 1 - np.array(div_db_full_ls)/np.array(div_b_full_ls)\n",
        "    m = np.mean(percent_decrease, axis=0)\n",
        "    s = np.std(percent_decrease, axis=0)\n",
        "    curr_ax.plot(results[run]['beta'], m, color=green, label='Biased', linewidth=lwd)\n",
        "    curr_ax.fill_between(results[run]['beta'], m - s, y2=m + s, color=green,\n",
        "                        alpha=0.25)\n",
        "\n",
        "    max_val = max(m + s)\n",
        "    eps = max_val/10\n",
        "    y_vals = [0, max_val/2, max_val]\n",
        "    y_vals_viz = np.round(np.array(y_vals)*100).astype(int)\n",
        "    curr_ax.set_ylim([0 - eps, max_val + eps])\n",
        "    curr_ax.set_yticks(y_vals)\n",
        "    curr_ax.set_yticklabels(y_vals_viz)\n",
        "    curr_ax.set_xticks([0, 0.5, 1])\n",
        "\n",
        "    if (d == 0):\n",
        "      curr_ax.set_ylabel('\\% Diversity\\nDecrease', fontsize=label_size)\n",
        "      curr_ax.get_yaxis().set_label_coords(-0.09,0.5)\n",
        "    curr_ax.tick_params(axis='both', which='major', labelsize=tick_size)\n",
        "\n",
        "    curr_ax.set_xlabel(r'$\\beta$', fontsize=label_size+3)\n",
        "\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.savefig(f'Debiased_{model}2.pdf', dpi=1200)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fPy-KM1IUBpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fYdvS0VsUCaE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}